CUDA HPL BENCHMARK


This text explain the step by step of installing the CUDA HPL Benchmark

// I continue in spanish and then i'll translate to english


1. Instalar dependencias

   1.1 OpenMPI
	
	Download openmpi-3.1.0.tar.gz
		wget https://download.open-mpi.org/release/open-mpi/v3.1/openmpi-3.1.0.tar.gz
	tar xvzf openmpi-3.1.0.tar.gz
	cd openmpi-3.1.0
	./configure --prefix=/usr/local/openmpi --with-cuda
	sudo make all install

	// Export the path, to have mpirun in /usr/local/openmpi/bin/mpirun //
	// Actulizacion: En multiples nodos se tiene que poner al inicio del ~/.bashrc //

	export PATH="$PATH:/usr/local/openmpi/bin"
	export LD_LIBRARY_PATH="LD_LIBRARY_PATH:/usr/local/openmpi/lib"

	// Para hacerlo persistente, toca ponerlo en ~/.bashrc //

   1.2 BLAS library: OpenBlas

	// Important! => Requieres to have installed "gfortran" before //

	Version library: https://sourceforge.net/projects/openblas/files/
	
	Download tar and uncompress it
		wget https://sourceforge.net/projects/openblas/files/v0.2.19/OpenBLAS%200.2.19%20version.tar.gz
		cp "OpenBLAS 0.2.19 version.tar.gz" OpenBLAS-0.2.19.tar.gz
		tar xvzf OpenBLAS-0.2.19.tar.gz
		cp -r xianyi-OpenBLAS-9a7e0ec/ OpenBLAS-0.2.19
		cd OpenBLAS-0.2.19
	make FC=gfortran
	sudo make PREFIX=/usr/local/openblas install

2. Modificar Make.CUDA de tal manera que se ajusten a los parametros de su sistema.

	(revisar el archivo ejemplo Make.CUDA)

   2.1 TOPdir

	TOPdir = $(HOME)/cloud/clusterSoCBench-master/HPL

   2.2 MPI Libraries

	MPdir        = /usr/local/openmpi
	MPinc        = -I$(MPdir)/include
	MPlib        = $(MPdir)/lib/libmpi.so

   2.3 BLAS Libraries (openBlas 0.2.19)

	LAdir        = /usr/local/openblas/
	LAinc        = -I$(LAdir)/include
	# CUDA
	LAlib	     = -L$(TOPdir)/src/cuda -ldgemm -L/usr/local/cuda/lib64 -lcuda -lcudart -lcublas -L$(LAdir)/lib/libopenblas.a $(LAdir)/lib/libopenblas_cortexa57p-r0.2.19.a

   2.4 Cuda Include (en mi caso no hay cambio, ya que esta en /usar/local/cuda)

	HPL_INCLUDES = -I$(INCdir) -I$(INCdir)/$(ARCH) $(LAinc) $(MPinc) -I/usr/local/cuda/include	

   2.5 Compilers and Linkers (wrapper es mpicc)

	CC      = /usr/local/openmpi/bin/mpicc
	CCFLAGS = $(HPL_DEFS) -fomit-frame-pointer -O3 -funroll-loops -W -Wall -fopenmp
	CCNOOPT      = $(HPL_DEFS) -O0 -w

3. (Opcional) Incluir informacion del movimiento de la memoria

    En <directorio proyecto>/src/cuda/makefile, descomentar "DEFINES += -DVERBOSE_PRINT".

    //Ojo: Baja el rendimiento del sistema en las pruebas por lo que no es muy recomendable.


4. Realizar el Make (no se necesita arch=CUDA ya que el make lo sabe)

	make

5. Ir a la carpeta ejecutable y modificar los valores del HPL.dat y ./run_linpack

	$ cd bin/CUDA
	$ nano HPL.dat
	$ nano run_linpack

   Lo anterior se revisa con los ejemplos en los archivos externos.

   5.1 HPL.dat

	Ns    -> Solo esta llegando a 5000 (por el problema de la RAM)
	P - Q -> Deben ser igual a la cantidad de CPU-GPU cores que tengan (2 nodos = 2 cpu-gpu)

   5.2 Run_linpack

	HPL_DIR=$HOME/cloud/clusterSoCBench-master/HPL	
	CPU_CORES_PER_GPU=1
	Comentar MKL_NUM_THREADS y GOTO_NUM_THREADS, ademas del export LD_LIBRARY_PATH.
	export CUDA_DGEMM_SPLIT=1.00
	export CUDA_DTRSM_SPLIT=1.00

6. Ejecutar la prueba

	mpirun -np 8 -host Tegra1,Tegra2,Tegra3,Tegra4,Tegra5,Tegra6,Tegra7,Tegra8 ./run_linpack 


LATER:

para gpu vision
http://nvidia.custhelp.com/app/answers/detail/a_id/3751/~/useful-nvidia-smi-queries
